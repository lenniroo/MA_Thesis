---
title: Master Thesis - Development of a Hate Crime Forecasting Model Based on Detected
  Hate Speech in Social Media
author: "Lennart Roesemeier"
date: "March 12, 2021"
output: html_document
---

```{r loading rquired packages}
if (!("rstudioapi" %in% installed.packages())) {
 install.packages("rstudioapi", dependencies = T)}
if (!("readr" %in% installed.packages())) {
 install.packages("readr", dependencies = T)}
if (!("stringi" %in% installed.packages())) {
 install.packages("stringi", dependencies = T)}
if (!("stringr" %in% installed.packages())) {
 install.packages("stringr", dependencies = T)}
if (!("tm" %in% installed.packages())) {
 install.packages("tm", dependencies = T)}
if (!("RTextTools" %in% installed.packages())) {
 install.packages("RTextTools", dependencies = T)}
if (!("tidyverse" %in% installed.packages())) {
 install.packages("tidyverse", dependencies = T)}

library(rstudioapi)
library(readr)
library(stringi)
library(stringr)
library(tm)
library(RTextTools)
library(tidyverse)

```

```{r}
#store the .Rmd-file as well as the .csv-files at your desired location
#by running this chunk you do not have to indicate your wd at commands like read/write.csv
wd <- rstudioapi::getActiveDocumentContext()$path
setwd(dirname(wd))
print(getwd())
```


```{r load hate speech data and merge it to one dataset}
data <- read.csv2("tweets_hateSpeech_2015-2020_0.csv", encoding = "UTF-8", sep = ",", stringsAsFactors = F)
data1 <- read.csv2("tweets_hateSpeech_2015-2020_1.csv", encoding = "UTF-8", sep = ",", stringsAsFactors = F)
data2 <- read.csv2("tweets_hateSpeech_2015-2020_2.csv", encoding = "UTF-8", sep = ",", stringsAsFactors = F)
data3 <- read.csv2("tweets_hateSpeech_2015-2020_3.csv", encoding = "UTF-8", sep = ",", stringsAsFactors = F)
data_user <- read.csv2("user_hateSpeech_2015-2020.csv", encoding = "UTF-8", sep = ",", stringsAsFactors = F)

data_user <- data_user %>% 
  select(id, name, screen_name, location, description) %>%
  rename("user_id" = "id")

tweets <- data %>%
  bind_rows(data1, data2, data3) %>%
  select(created_at, full_text, user_id, user_id_str) %>%
  left_join(data_user, by = "user_id")

write.csv(tweets, "tweets_hateSpeech.csv", row.names = F) #dirty data

rm(data, data1, data2, data3, data_user)
```

```{r load tweets}
#tweets <- read.csv2("tweets_hateSpeech.csv", encoding = "UTF-8", sep = ",", stringsAsFactors = F)
```

```{r check the encoding with small sample since it's German Twitter Data (reasoned by umlauts Ä, Ö, Ü, and ä, ö, ü)}
data_n <- tweets %>%
  slice(1:25)

stri_enc_mark(data_n$full_text) #UTF-8 and ASCII
all(stri_enc_isutf8(data_n$full_text)) #True = it's possible to encode
```

```{r encode umlauts}
rm(data_n)
#if the this file isn't open with UTF-8 encoding you have to replace the umlauts decoded #signs (e.g. Ã¤) with the actual UTF-8 encoded ones (from e.g. my source: #https://www.i18nqa.com/debug/utf8-debug.html) otherwise it doesn't work
encode_umlauts <- function(x) {
  x <- stringr::str_replace_all(x, c('Ã„' = "Ae", "Ã–" = "Oe", "Ãœ" = "Ue",
                            "Ã¤" = "ae", "Ã¶" = "oe", "Ã¼" = "ue"))
}

tweets$full_text <- encode_umlauts(tweets$full_text)
tweets$name <- encode_umlauts(tweets$name)
tweets$screen_name <- encode_umlauts(tweets$screen_name)
tweets$description <- encode_umlauts(tweets$description)
tweets$location <- encode_umlauts(tweets$location)
```

```{r encode coordinates and get rid of all other non-alphanumerical characters}
encode_coordinates <- function(x) {
  x <- gsub("Â°", "°" , x)
  x <- gsub("â€²", "", x)
  x <- gsub("â€³", "", x)
}

tweets$location <- encode_coordinates(tweets$location)
tweets$location <- str_replace_all(tweets$location,
                c("ÃY" = "s", "[\r\n]" = " ", 
                  "[[:punct:]]" = " ", "[^a-zA-Z0-9°]" = " "))
```

```{r get rid of URLs}
removeURL <- function(x) gsub("(f|ht)tp(s?)://\\S+", "", x, perl=T) 

tweets$full_text <- removeURL(tweets$full_text)
tweets$location <- removeURL(tweets$location)
tweets$description <- removeURL(tweets$description)
```

```{r get rid of usernames in Tweets and descriptions}
remove_usernames <- function(x) gsub("@\\w+", "", x)

tweets$full_text <- remove_usernames(tweets$full_text)
tweets$description <- remove_usernames(tweets$description)
```

```{r encode ß, line break, and get rid of all non-alphanumerical characters}
clean_tweets <- function(x) {
  x <- gsub("ÃŸ", "s", x)
  x <- gsub("[\r\n]", " ", x)
  x <- gsub("[[:punct:]]", " ", x)
  x <- gsub("[^[:alnum:] ]", " ", x)
  x <- gsub("[^a-zA-Z0-9]", " ", x)
  x <- gsub("amp", "", x)
}

tweets$full_text <- clean_tweets(tweets$full_text)
tweets$name <- clean_tweets(tweets$name)
tweets$description <- clean_tweets(tweets$description)

tweets$full_text <- str_replace(tweets$full_text, "gt", "")
```

```{r store clean Tweets as csv-file}
#write.csv(tweets, "tweets_hateSpeech_clean.csv", row.names = F, fileEncoding = "utf-8")
```

```{r final touch of cleaning}
corpus <- iconv(tweets$full_text)
corpus <- Corpus(VectorSource(corpus))

tweets_clean <- tm_map(corpus, stripWhitespace)
twee <- data.frame(t = sapply(tweets_clean, as.character), stringsAsFactors = F)
tweets$full_text <- twee$t

rm(corpus,tweets_clean,twee)
```


```{r loading training hate speech dataset by Ross et al}
hateURL <- 'https://raw.githubusercontent.com/UCSM-DUE/IWG_hatespeech_public/master/german%20hatespeech%20refugees.csv'
hate_speech <- read.csv(hateURL, encoding = 'utf-8')

#hate_speech %>% 
#  filter(HatespeechOrNot..Expert.1. == "YES" | HatespeechOrNot..Expert.2. == "YES") %>% 
#  count() #154

#hate_speech %>% 
#  filter(HatespeechOrNot..Expert.1. == "YES" & HatespeechOrNot..Expert.2. == "YES") %>% 
#  count() #54

#hate_speech %>% 
#  filter(HatespeechOrNot..Expert.1. == "NO" & HatespeechOrNot..Expert.2. == "NO") %>% 
#  count() #315
```

```{r clean Tweets in training dataset}
hate_speech$Tweet <- encode_umlauts(hate_speech$Tweet)
hate_speech$Tweet <- clean_tweets(hate_speech$Tweet)
```

```{r labeling hate speech: 1 - "hate speech", 0 - "no hate speech"}
hate_speech <- hate_speech %>%
  mutate(label = if_else(HatespeechOrNot..Expert.1. == "NO" & HatespeechOrNot..Expert.2. == "NO",0,1))
```


```{r}
set.seed(1312)

samp_train <- sample(1:nrow(hate_speech),
               round(nrow(hate_speech)*.70),
               replace = F)

tr <- hate_speech[samp_train, ]
te <- hate_speech[-samp_train, ]

hate_speech <- rbind(tr,te)

tr <- tr[ , -c(2:4)]
te <- te[ , -c(2:4)]

train <- rbind(tr,te)
```

```{r train SVM with Ross et al. 2017 data}
corpus <- Corpus(VectorSource(train$Tweet))
dtm <- DocumentTermMatrix(corpus, 
                          control = list(weighting = 
                                           function(x)
                                             weightTfIdf(x, normalize = F)))

train_codes = train$label

container <- create_container(dtm, 
                              t(train_codes),
                              trainSize = 1:nrow(tr),
                              testSize = (nrow(tr)+1):nrow(train),
                              virgin = F)

models <- train_models(container, algorithms = c("SVM", "TREE", "BAGGING", "BOOSTING", "RF"), kernel = "radial", cost = 1)

results <- classify_models(container, models)

out <- data.frame(label_svm = results$SVM_LABEL,
                  prob_svm = results$SVM_PROB,
                  label_tree = results$TREE_LABEL,
                  prob_tree = results$TREE_PROB,
                  label_bagging = results$BAGGING_LABEL,
                  prob_bagging = results$BAGGING_PROB,
                  label_boosting = results$LOGITBOOST_LABEL,
                  prob_boosting = results$LOGITBOOST_PROB,
                  label_rf = results$FORESTS_LABEL,
                  prob_rf = results$FORESTS_PROB,
                  actual_label = train$label[(nrow(tr)+1):nrow(train)])

(z_svm = as.matrix(table(out[,1], out[,11])))
(z_tree = as.matrix(table(out[,3], out[,11])))
(z_bagging = as.matrix(table(out[,5], out[,11])))
(z_boosting = as.matrix(table(out[,7], out[,11])))
(z_rf = as.matrix(table(out[,9], out[,11])))

(pct_svm = round(((z_svm[1,1] + z_svm[2,2]) / sum(z_svm)) * 100, 2))                      #64.54% accuracy
(pct_tree = round(((z_tree[1,1] + z_tree[2,2]) / sum(z_tree)) * 100, 2))                  #63.83% accuracy
(pct_bagging = round(((z_bagging[1,1] + z_bagging[2,2]) / sum(z_bagging)) * 100, 2))      #70.21% accuracy
(pct_boosting = round(((z_boosting[1,1] + z_boosting[2,2]) / sum(z_boosting)) * 100, 2))  #66.67% accuracy
(pct_rf = round(((z_rf[1,1] + z_rf[2,2]) / sum(z_rf)) * 100, 2))                          #65.96% accuracy

#rm(tr,te)
```




```{r}
set.seed(420)

tweets <- tweets %>%
  distinct()

samp <- sample(1:nrow(tweets),
               round(nrow(tweets)*.50),
               replace = F)
random1 <- tweets[samp, ]
random2 <- tweets[-samp, ]

tweets <- rbind(random1,random2)

test <- tweets %>%
  select(full_text) %>%
  rename(Tweet = full_text) %>%
  mutate(label = "") %>%
  na_if("")

#random_t <- rbind(train,test)

rm(random1,random2)
```

```{r SVM loop on test data}
c <- 1:777
index <- rep(c, each = 132)
test1 <- test[1:102564, ]
test1 <- cbind(test1,index)

l <- list()

count <- 1

for (i in c) {
  
  cat(i)
  
  if(count %in% test1$index) {
    df <- test1[which(test1$index == count), ]
    
    corpus = Corpus(VectorSource(df$Tweet))
    dtm_test = DocumentTermMatrix(corpus, control = list(weighting = 
                                                           function(x)
                                                             weightTfIdf(x, normalize = F)))
    
    row.names(dtm_test) = (nrow(dtm)+1):(nrow(dtm)+nrow(dtm_test))
    
    dtm_p = c(dtm, dtm_test)   
    
    train_codes_p = c(train_codes, 
                      rep(NA, length(df))) 
    
    container_p <- create_container(dtm_p,
                                    t(train_codes_p),
                                    trainSize = 1:nrow(dtm),
                                    testSize = (nrow(dtm)+1):(nrow(dtm)+nrow(dtm_test)),
                                    virgin = T)
    
    model_p <- train_models(container_p, algorithms = c("SVM"), kernel = "radial", cost = 1)
    
    prediction <- classify_models(container_p, model_p)
    
    out_p <- data.frame(model_label = prediction$SVM_LABEL,
                        model_prob = prediction$SVM_PROB,
                        actual = df)
    
  }
  
  l[[i]] <- out_p
  combined <- bind_rows(l)
  
  count <- count + 1
}

write.csv(combined, "wave1_SVM.csv", row.names = F, fileEncoding = "utf-8")
```


```{r}
test1 <- test[102565:205128, ]
test1 <- cbind(test1,index)

l <- list()

count <- 1

for (i in c) {
  
  cat(i)
  
  if(count %in% test1$index) {
    df <- test1[which(test1$index == count), ]
    
    corpus = Corpus(VectorSource(df$Tweet))
    dtm_test = DocumentTermMatrix(corpus, control = list(weighting = 
                                                           function(x)
                                                             weightTfIdf(x, normalize = F)))
    
    row.names(dtm_test) = (nrow(dtm)+1):(nrow(dtm)+nrow(dtm_test))
    
    dtm_p = c(dtm, dtm_test)   
    
    train_codes_p = c(train_codes, 
                      rep(NA, length(df))) 
    
    container_p <- create_container(dtm_p,
                                    t(train_codes_p),
                                    trainSize = 1:nrow(dtm),
                                    testSize = (nrow(dtm)+1):(nrow(dtm)+nrow(dtm_test)),
                                    virgin = T)
    
    model_p <- train_models(container_p, algorithms = c("SVM"), kernel = "radial", cost = 1)
    
    prediction <- classify_models(container_p, model_p)
    
    out_p <- data.frame(model_label = prediction$SVM_LABEL,
                        model_prob = prediction$SVM_PROB,
                        actual = df)
    
  }
  
  l[[i]] <- out_p
  combined2 <- bind_rows(l)
  
  count <- count + 1
}

combined <- rbind(combined,combined2)
write.csv(combined, "wave2_SVM.csv", row.names = F, fileEncoding = "utf-8")
rm(combined2)
```

```{r}
test1 <- test[205129:307692, ]
test1 <- cbind(test1,index)

l <- list()

count <- 1

for (i in c) {
  
  cat(i)
  
  if(count %in% test1$index) {
    df <- test1[which(test1$index == count), ]
    
    corpus = Corpus(VectorSource(df$Tweet))
    dtm_test = DocumentTermMatrix(corpus, control = list(weighting = 
                                                           function(x)
                                                             weightTfIdf(x, normalize = F)))
    
    row.names(dtm_test) = (nrow(dtm)+1):(nrow(dtm)+nrow(dtm_test))
    
    dtm_p = c(dtm, dtm_test)   
    
    train_codes_p = c(train_codes, 
                      rep(NA, length(df))) 
    
    container_p <- create_container(dtm_p,
                                    t(train_codes_p),
                                    trainSize = 1:nrow(dtm),
                                    testSize = (nrow(dtm)+1):(nrow(dtm)+nrow(dtm_test)),
                                    virgin = T)
    
    model_p <- train_models(container_p, algorithms = c("SVM"), kernel = "radial", cost = 1)
    
    prediction <- classify_models(container_p, model_p)
    
    out_p <- data.frame(model_label = prediction$SVM_LABEL,
                        model_prob = prediction$SVM_PROB,
                        actual = df)
    
  }
  
  l[[i]] <- out_p
  combined3 <- bind_rows(l)
  
  count <- count + 1
}

combined <- rbind(combined,combined3)
write.csv(combined, "wave3_SVM.csv", row.names = F, fileEncoding = "utf-8")
rm(combined3)
```

```{r}
test1 <- test[307693:410256, ]
test1 <- cbind(test1,index)

l <- list()

count <- 1

for (i in c) {
  
  cat(i)
  
  if(count %in% test1$index) {
    df <- test1[which(test1$index == count), ]
    
    corpus = Corpus(VectorSource(df$Tweet))
    dtm_test = DocumentTermMatrix(corpus, control = list(weighting = 
                                                           function(x)
                                                             weightTfIdf(x, normalize = F)))
    
    row.names(dtm_test) = (nrow(dtm)+1):(nrow(dtm)+nrow(dtm_test))
    
    dtm_p = c(dtm, dtm_test)   
    
    train_codes_p = c(train_codes, 
                      rep(NA, length(df))) 
    
    container_p <- create_container(dtm_p,
                                    t(train_codes_p),
                                    trainSize = 1:nrow(dtm),
                                    testSize = (nrow(dtm)+1):(nrow(dtm)+nrow(dtm_test)),
                                    virgin = T)
    
    model_p <- train_models(container_p, algorithms = c("SVM"), kernel = "radial", cost = 1)
    
    prediction <- classify_models(container_p, model_p)
    
    out_p <- data.frame(model_label = prediction$SVM_LABEL,
                        model_prob = prediction$SVM_PROB,
                        actual = df)
    
  }
  
  l[[i]] <- out_p
  combined4 <- bind_rows(l)
  
  count <- count + 1
}

combined <- rbind(combined,combined4)
write.csv(combined, "wave4_SVM.csv", row.names = F, fileEncoding = "utf-8")
rm(combined4)
```

```{r}
test1 <- test[410257:512820, ]
test1 <- cbind(test1,index)

l <- list()

count <- 1

for (i in c) {
  
  cat(i)
  
  if(count %in% test1$index) {
    df <- test1[which(test1$index == count), ]
    
    corpus = Corpus(VectorSource(df$Tweet))
    dtm_test = DocumentTermMatrix(corpus, control = list(weighting = 
                                                           function(x)
                                                             weightTfIdf(x, normalize = F)))
    
    row.names(dtm_test) = (nrow(dtm)+1):(nrow(dtm)+nrow(dtm_test))
    
    dtm_p = c(dtm, dtm_test)   
    
    train_codes_p = c(train_codes, 
                      rep(NA, length(df))) 
    
    container_p <- create_container(dtm_p,
                                    t(train_codes_p),
                                    trainSize = 1:nrow(dtm),
                                    testSize = (nrow(dtm)+1):(nrow(dtm)+nrow(dtm_test)),
                                    virgin = T)
    
    model_p <- train_models(container_p, algorithms = c("SVM"), kernel = "radial", cost = 1)
    
    prediction <- classify_models(container_p, model_p)
    
    out_p <- data.frame(model_label = prediction$SVM_LABEL,
                        model_prob = prediction$SVM_PROB,
                        actual = df)
    
  }
  
  l[[i]] <- out_p
  combined5 <- bind_rows(l)
  
  count <- count + 1
}

combined <- rbind(combined,combined5)
write.csv(combined, "wave5_SVM.csv", row.names = F, fileEncoding = "utf-8")
rm(combined5)
```

```{r}
test1 <- test[512821:615384, ]
test1 <- cbind(test1,index)

l <- list()

count <- 1

for (i in c) {
  
  cat(i)
  
  if(count %in% test1$index) {
    df <- test1[which(test1$index == count), ]
    
    corpus = Corpus(VectorSource(df$Tweet))
    dtm_test = DocumentTermMatrix(corpus, control = list(weighting = 
                                                           function(x)
                                                             weightTfIdf(x, normalize = F)))
    
    row.names(dtm_test) = (nrow(dtm)+1):(nrow(dtm)+nrow(dtm_test))
    
    dtm_p = c(dtm, dtm_test)   
    
    train_codes_p = c(train_codes, 
                      rep(NA, length(df))) 
    
    container_p <- create_container(dtm_p,
                                    t(train_codes_p),
                                    trainSize = 1:nrow(dtm),
                                    testSize = (nrow(dtm)+1):(nrow(dtm)+nrow(dtm_test)),
                                    virgin = T)
    
    model_p <- train_models(container_p, algorithms = c("SVM"), kernel = "radial", cost = 1)
    
    prediction <- classify_models(container_p, model_p)
    
    out_p <- data.frame(model_label = prediction$SVM_LABEL,
                        model_prob = prediction$SVM_PROB,
                        actual = df)
    
  }
  
  l[[i]] <- out_p
  combined6 <- bind_rows(l)
  
  count <- count + 1
}

combined <- rbind(combined,combined6)
write.csv(combined, "wave6_SVM.csv", row.names = F, fileEncoding = "utf-8")
rm(combined6)
```

```{r}
test1 <- test[615385:717948, ]
test1 <- cbind(test1,index)

l <- list()

count <- 1

for (i in c) {
  
  cat(i)
  
  if(count %in% test1$index) {
    df <- test1[which(test1$index == count), ]
    
    corpus = Corpus(VectorSource(df$Tweet))
    dtm_test = DocumentTermMatrix(corpus, control = list(weighting = 
                                                           function(x)
                                                             weightTfIdf(x, normalize = F)))
    
    row.names(dtm_test) = (nrow(dtm)+1):(nrow(dtm)+nrow(dtm_test))
    
    dtm_p = c(dtm, dtm_test)   
    
    train_codes_p = c(train_codes, 
                      rep(NA, length(df))) 
    
    container_p <- create_container(dtm_p,
                                    t(train_codes_p),
                                    trainSize = 1:nrow(dtm),
                                    testSize = (nrow(dtm)+1):(nrow(dtm)+nrow(dtm_test)),
                                    virgin = T)
    
    model_p <- train_models(container_p, algorithms = c("SVM"), kernel = "radial", cost = 1)
    
    prediction <- classify_models(container_p, model_p)
    
    out_p <- data.frame(model_label = prediction$SVM_LABEL,
                        model_prob = prediction$SVM_PROB,
                        actual = df)
    
  }
  
  l[[i]] <- out_p
  combined7 <- bind_rows(l)
  
  count <- count + 1
}

combined <- rbind(combined,combined7)
write.csv(combined, "wave7_SVM.csv", row.names = F, fileEncoding = "utf-8")
rm(combined7)
```

```{r}
test1 <- test[717949:820512, ]
test1 <- cbind(test1,index)

l <- list()

count <- 1

for (i in c) {
  
  cat(i)
  
  if(count %in% test1$index) {
    df <- test1[which(test1$index == count), ]
    
    corpus = Corpus(VectorSource(df$Tweet))
    dtm_test = DocumentTermMatrix(corpus, control = list(weighting = 
                                                           function(x)
                                                             weightTfIdf(x, normalize = F)))
    
    row.names(dtm_test) = (nrow(dtm)+1):(nrow(dtm)+nrow(dtm_test))
    
    dtm_p = c(dtm, dtm_test)   
    
    train_codes_p = c(train_codes, 
                      rep(NA, length(df))) 
    
    container_p <- create_container(dtm_p,
                                    t(train_codes_p),
                                    trainSize = 1:nrow(dtm),
                                    testSize = (nrow(dtm)+1):(nrow(dtm)+nrow(dtm_test)),
                                    virgin = T)
    
    model_p <- train_models(container_p, algorithms = c("SVM"), kernel = "radial", cost = 1)
    
    prediction <- classify_models(container_p, model_p)
    
    out_p <- data.frame(model_label = prediction$SVM_LABEL,
                        model_prob = prediction$SVM_PROB,
                        actual = df)
    
  }
  
  l[[i]] <- out_p
  combined8 <- bind_rows(l)
  
  count <- count + 1
}

combined <- rbind(combined,combined8)
write.csv(combined, "wave8_SVM.csv", row.names = F, fileEncoding = "utf-8")
rm(combined8)
```

```{r}
c <- 1:6
index <- rep(c, each = 63)
test1 <- test[820513:820890, ]
test1 <- cbind(test1,index)

l <- list()

count <- 1

for (i in c) {
  
  cat(i)
  
  if(count %in% test1$index) {
    df <- test1[which(test1$index == count), ]
    
    corpus = Corpus(VectorSource(df$Tweet))
    dtm_test = DocumentTermMatrix(corpus, control = list(weighting = 
                                                           function(x)
                                                             weightTfIdf(x, normalize = F)))
    
    row.names(dtm_test) = (nrow(dtm)+1):(nrow(dtm)+nrow(dtm_test))
    
    dtm_p = c(dtm, dtm_test)   
    
    train_codes_p = c(train_codes, 
                      rep(NA, length(df))) 
    
    container_p <- create_container(dtm_p,
                                    t(train_codes_p),
                                    trainSize = 1:nrow(dtm),
                                    testSize = (nrow(dtm)+1):(nrow(dtm)+nrow(dtm_test)),
                                    virgin = T)
    
    model_p <- train_models(container_p, algorithms = c("SVM"), kernel = "radial", cost = 1)
    
    prediction <- classify_models(container_p, model_p)
    
    out_p <- data.frame(model_label = prediction$SVM_LABEL,
                        model_prob = prediction$SVM_PROB,
                        actual = df)
    
  }
  
  l[[i]] <- out_p
  combined9 <- bind_rows(l)
  
  count <- count + 1
}

combined <- rbind(combined,combined9)
write.csv(combined, "predictedHateSpeechTweets_bySVM.csv", row.names = F, fileEncoding = "utf-8")
rm(combined9)
```



```{r}
tweets <- tweets[1:820890, ]

tw <- tweets %>%
  rename(Tweet = full_text)

com <- combined %>%
  select(model_label, model_prob, actual.Tweet) 

all <- cbind(tw, com)

all <- all[, -c(4,11)]

write.csv(all, "ClassifiedTweets.csv", row.names = F, fileEncoding = "utf-8")

all %>%
  filter(model_label == 1) %>%
  count() #40257

rm(com,tw,container,container_p,corpus,
   df,dtm,dtm_p,dtm_test,l,model_p,models,
   out,out_p,prediction,results,test1,c,
   count,i,index,pct,samp,samp_train,
   train_codes,train_codes_p,z,hate_speech,
   hateURL,test,train,combined,tweets)
```

```{r}
hate <- all %>%
  filter(model_label == 1)

hate <- hate %>%
  mutate_at(vars(model_label, model_prob), as.numeric) %>%
  arrange(desc(model_prob))

summary(hate) #3rd Quantil = 0.6067

#greater than the 3rd Quantil (chance to classify right above 60%)
hate_new <- hate %>%
  filter(model_prob > 0.6) #11093
```





