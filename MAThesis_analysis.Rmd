---
title: Master Thesis - Development of a Hate Crime Forecasting Model Based on Detected
  Hate Speech in Social Media
author: "Lennart Roesemeier"
date: "March 12, 2021"
output: html_document
---

```{r loading rquired packages}
#if (!require(dplyr)) install.packages("dplyr")
#if (!require(raedr)) install.packages("readr", dependencies = T)
#if (!require(stringi)) install.packages("stringi", dependencies = T)
#if (!require(stringr)) install.packages("stringr", dependencies = T)
#if (!require(tm)) install.packages("tm", dependencies = T)
#if (!require(e1071)) install.packages("e1071", dependencies = T)
#if (!require(kernlab)) install.packages("kernlab", dependencies = T)
#if (!require(RTextTools)) install.packages("RTextTools", dependencies = T)
#if (!require(tidyverse)) install.packages("tidyverse", dependencies = T)

library(tidyverse)
library(readr)
library(dplyr)
library(stringi)
library(stringr)
library(tm)
library(RTextTools)
library(e1071)

```

```{r load hate speech data and merge it to one dataset}
data <- read.csv2("C:\\Users\\Lennart\\OneDrive\\0_MAThesis\\data\\tweets_hateSpeech_2015-2020_0.csv", encoding = "UTF-8", sep = ",", stringsAsFactors = F)
data1 <- read.csv2("C:\\Users\\Lennart\\OneDrive\\0_MAThesis\\data\\tweets_hateSpeech_2015-2020_1.csv", encoding = "UTF-8", sep = ",", stringsAsFactors = F)
data2 <- read.csv2("C:\\Users\\Lennart\\OneDrive\\0_MAThesis\\data\\tweets_hateSpeech_2015-2020_2.csv", encoding = "UTF-8", sep = ",", stringsAsFactors = F)
data3 <- read.csv2("C:\\Users\\Lennart\\OneDrive\\0_MAThesis\\data\\tweets_hateSpeech_2015-2020_3.csv", encoding = "UTF-8", sep = ",", stringsAsFactors = F)
data_user <- read.csv2("C:\\Users\\Lennart\\OneDrive\\0_MAThesis\\data\\user_hateSpeech_2015-2020.csv", encoding = "UTF-8", sep = ",", stringsAsFactors = F)

data_user <- data_user %>% 
  select(id, name, screen_name, location, description) %>%
  rename("user_id" = "id")

tweets <- data %>%
  bind_rows(data1, data2, data3) %>%
  select(created_at, full_text, user_id, user_id_str) %>%
  left_join(data_user, by = "user_id")

#write.csv(tweets, "C:\\Users\\Lennart\\OneDrive\\0_MAThesis\\data\\tweets_hateSpeech.csv", row.names = F) #dirty data

rm(data, data1, data2, data3, data_user)
```

```{r load tweets}
#tweets <- read.csv2("C:\\Users\\Lennart\\OneDrive\\0_MAThesis\\data\\tweets_hateSpeech.csv", encoding = "UTF-8", sep = ",", stringsAsFactors = F)
```

```{r check the encoding with small sample since it's German Twitter Data (reasoned by umlauts Ä, Ö, Ü, and ä, ö, ü)}
data_n <- tweets %>%
  slice(1:25)

stri_enc_mark(data_n$full_text) #UTF-8 and ASCII
all(stri_enc_isutf8(data_n$full_text)) #True = it's possible to encode
```

```{r encode umlauts}
rm(data_n)
#if the this file isn't open with UTF-8 encoding you have to replace the umlauts decoded #signs (e.g. Ã¤) with the actual UTF-8 encoded ones (from e.g. my source: #https://www.i18nqa.com/debug/utf8-debug.html) otherwise it doesn't work
encode_umlauts <- function(x) {
  x <- stringr::str_replace_all(x, c('Ã„' = "Ae", "Ã–" = "Oe", "Ãœ" = "Ue",
                            "Ã¤" = "ae", "Ã¶" = "oe", "Ã¼" = "ue"))
}

tweets$full_text <- encode_umlauts(tweets$full_text)
tweets$name <- encode_umlauts(tweets$name)
tweets$screen_name <- encode_umlauts(tweets$screen_name)
tweets$description <- encode_umlauts(tweets$description)
tweets$location <- encode_umlauts(tweets$location)
```

```{r encode coordinates and get rid of all other non-alphanumerical characters}
encode_coordinates <- function(x) {
  x <- gsub("Â°", "°" , x)
  x <- gsub("â€²", "", x)
  x <- gsub("â€³", "", x)
}

tweets$location <- encode_coordinates(tweets$location)
tweets$location <- str_replace_all(tweets$location,
                c("ÃY" = "s", "[\r\n]" = " ", 
                  "[[:punct:]]" = " ", "[^a-zA-Z0-9°]" = " "))
```

```{r get rid of URLs}
removeURL <- function(x) gsub("(f|ht)tp(s?)://\\S+", "", x, perl=T) 

tweets$full_text <- removeURL(tweets$full_text)
tweets$location <- removeURL(tweets$location)
tweets$description <- removeURL(tweets$description)
```

```{r get rid of usernames in Tweets and descriptions}
remove_usernames <- function(x) gsub("@\\w+", "", x)

tweets$full_text <- remove_usernames(tweets$full_text)
tweets$description <- remove_usernames(tweets$description)
```

```{r encode ß, line break, and get rid of all non-alphanumerical characters}
clean_tweets <- function(x) {
  x <- gsub("ÃŸ", "s", x)
  x <- gsub("[\r\n]", " ", x)
  x <- gsub("[[:punct:]]", " ", x)
  x <- gsub("[^[:alnum:] ]", " ", x)
  x <- gsub("[^a-zA-Z0-9]", " ", x)
  x <- gsub("amp", "", x)
}

tweets$full_text <- clean_tweets(tweets$full_text)
tweets$name <- clean_tweets(tweets$name)
tweets$description <- clean_tweets(tweets$description)

tweets$full_text <- str_replace(tweets$full_text, "gt", "")
```

```{r store clean Tweets as csv-file}
#write.csv(tweets, "C:\\Users\\Lennart\\OneDrive\\0_MAThesis\\data\\tweets_hateSpeech_clean.csv", row.names = F, fileEncoding = "utf-8")
```

```{r final touch of cleaning}
corpus <- iconv(tweets$full_text)
corpus <- Corpus(VectorSource(corpus))

tweets_clean <- tm_map(corpus, stripWhitespace)
twee <- data.frame(t = sapply(tweets_clean, as.character), stringsAsFactors = F)
tweets$full_text <- twee$t

rm(corpus,tweets_clean,twee)
```


```{r loading training hate speech dataset by Ross et al}
hateURL <- 'https://raw.githubusercontent.com/UCSM-DUE/IWG_hatespeech_public/master/german%20hatespeech%20refugees.csv'
hate_speech <- read.csv(hateURL, encoding = 'utf-8')

#hate_speech %>% 
#  filter(HatespeechOrNot..Expert.1. == "YES" | HatespeechOrNot..Expert.2. == "YES") %>% 
#  count() #154

#hate_speech %>% 
#  filter(HatespeechOrNot..Expert.1. == "YES" & HatespeechOrNot..Expert.2. == "YES") %>% 
#  count() #54

#hate_speech %>% 
#  filter(HatespeechOrNot..Expert.1. == "NO" & HatespeechOrNot..Expert.2. == "NO") %>% 
#  count() #315
```

```{r clean Tweets in training dataset}
hate_speech$Tweet <- encode_umlauts(hate_speech$Tweet)
hate_speech$Tweet <- clean_tweets(hate_speech$Tweet)
```

```{r labeling hate speech: 1 - "hate speech", 0 - "no hate speech"}
hate_speech <- hate_speech %>%
  mutate(label = if_else(HatespeechOrNot..Expert.1. == "NO" & HatespeechOrNot..Expert.2. == "NO",0,1))
```






```{r}
train_t <- hate_speech %>%
  select(Tweet, label)

tr <- train_t[1:279, ]
te <- train_t[280:469, ]

test_t <- tweets %>%
  select(full_text) %>%
  rename(Tweet = full_text) %>%
  mutate(label = "") %>%
  na_if("")

test_test <- test_t[1:82080, ]

#random_t <- rbind(train_t,test_t)

random_test <- rbind(train_t,test_test)
```

```{r train SVM with Ross et al. data}
corpus <- Corpus(VectorSource(train_t$Tweet))
dtm <- DocumentTermMatrix(corpus, 
                          control = list(weighting = 
                                           function(x)
                                             weightTfIdf(x, normalize = F)))

train_codes = train_t$label

container <- create_container(dtm, 
                              t(train_codes),
                              trainSize = 1:nrow(tr),
                              testSize = (nrow(tr)+1):nrow(train_t),
                              virgin = F)

models <- train_models(container, algorithms = c("SVM"), kernel = "radial", cost = 1)

results <- classify_models(container, models)

out <- data.frame(model_label = results$SVM_LABEL,
                  model_prob = results$SVM_PROB,
                  actual_label = train_t$label[(nrow(tr)+1):nrow(train_t)])

(z = as.matrix(table(out[,1], out[,3])))

(pct = round(((z[1,1] + z[2,2]) / sum(z)) * 100, 2)) #~ 70% accuracy


```


```{r SVM loop on test data}
c <- 1:432
index <- rep(c, each = 190)
test_test <- cbind(test_test,index)

l <- list()

count <- 1

for (i in c) {
  if(count %in% test_test$index) {
    df <- test_test[which(test_test$index == count), ]
    
    corpus = Corpus(VectorSource(df$Tweet))
    dtm_test = DocumentTermMatrix(corpus, control = list(weighting = 
                                                           function(x)
                                                             weightTfIdf(x, normalize = F)))
    
    row.names(dtm_test) = (nrow(dtm)+1):(nrow(dtm)+nrow(dtm_test))
    
    dtm_p = c(dtm, dtm_test)   
    
    train_codes_p = c(train_codes, 
                      rep(NA, length(df))) 
    
    container_p <- create_container(dtm_p,
                                    t(train_codes_p),
                                    trainSize = 1:nrow(dtm),
                                    testSize = (nrow(dtm)+1):(nrow(dtm)+nrow(dtm_test)),
                                    virgin = T)
    
    model_p <- train_models(container_p, algorithms = c("SVM"), kernel = "radial", cost = 1)
    
    prediction <- classify_models(container_p, model_p)
    
    out_p <- data.frame(model_label = prediction$SVM_LABEL,
                        model_prob = prediction$SVM_PROB,
                        actual = df)
    
  }
  
  l[[i]] <- out_p
  combined <- bind_rows(l)
  
  count <- count + 1
}



#perhaps it's possible to create a new column with indicies and sample over the index criteria, index = 1:4321, 190 steps; take the first 190 apply svm bind rows, take the second 190 apply svm bind rows, take the third 190 apply svm bind rows, ...

```

```{r}
combined %>%
  filter(model_label == 1) %>%
  count() #10% sample ~ 1814 hate speech/offensive language
```



